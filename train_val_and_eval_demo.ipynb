{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from catr.configuration import Config\n",
    "from catr.models.utils import NestedTensor, nested_tensor_from_tensor_list, get_rank\n",
    "from catr.models.backbone import build_backbone\n",
    "from catr.models.transformer import build_transformer\n",
    "from catr.models.position_encoding import PositionEmbeddingSine\n",
    "from catr.models.caption import MLP\n",
    "\n",
    "import json\n",
    "\n",
    "from dataset.dataset import ImageFeatureDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_ethan import *\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), \"catr\"))\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.load(\"glove_embed.npy\")\n",
    "with open('word2ind.json') as json_file: \n",
    "    word2ind = json.load(json_file) \n",
    "with open('ind2word.json') as json_file: \n",
    "    ind2word = json.load(json_file) \n",
    "config = Config()\n",
    "config.device = 'cpu' # if running without GPU\n",
    "config.feature_dim = 1024\n",
    "config.pad_token_id = word2ind[\"<S>\"]\n",
    "config.hidden_dim = 300\n",
    "config.nheads = 10\n",
    "config.batch_size = 8\n",
    "config.encoder_type = 1\n",
    "config.vocab_size = words.shape[0]\n",
    "config.dir = '../mimic_features'\n",
    "config.__dict__[\"pre_embed\"] = torch.from_numpy(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Device: cpu\n",
      "Number of params: 33908144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Xray_Captioner(\n",
       "  (input_proj): Conv2d(1024, 300, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (position_embedding): PositionEmbeddingSine()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (embeddings): DecoderEmbeddings(\n",
       "      (word_embeddings): Embedding(23100, 300, padding_idx=1678)\n",
       "      (position_embeddings): Embedding(128, 300)\n",
       "      (LayerNorm): LayerNorm((300,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=300, out_features=512, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (2): Linear(in_features=512, out_features=23100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, criterion = main(config)\n",
    "model = model.float()\n",
    "device = torch.device(config.device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dicts = [\n",
    "        {\"params\": [p for n, p in model.named_parameters(\n",
    "        ) if \"backbone\" not in n and p.requires_grad]},\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "            \"lr\": config.lr_backbone,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "        param_dicts, lr=config.lr, weight_decay=config.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config.lr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 128\n",
      "Val: 32\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ImageFeatureDataset(config, mode='train')\n",
    "dataset_val = ImageFeatureDataset(config, mode='val')\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, config.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "        dataset_train, batch_sampler=batch_sampler_train, num_workers=config.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, config.batch_size,\n",
    "                                 sampler=sampler_val, drop_last=False, num_workers=config.num_workers)\n",
    "print(f\"Train: {len(dataset_train)}\")\n",
    "print(f\"Val: {len(dataset_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(config.checkpoint):\n",
    "     print(\"Loading Checkpoint...\")\n",
    "     checkpoint = torch.load(config.checkpoint, map_location='cpu')\n",
    "     model.load_state_dict(checkpoint['model'])\n",
    "     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "     lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "     config.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(\"Start Training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8ec41115a3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     epoch_loss = train_one_epoch(\n\u001b[0m\u001b[1;32m      7\u001b[0m         model, criterion, data_loader_train, optimizer, device, epoch, config.clip_max_norm)\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/xray/xrayreportgeneration/catr/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "\n",
    "for epoch in range(config.start_epoch, config.epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    epoch_loss = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch, config.clip_max_norm)\n",
    "    train_loss_hist.append(epoch_loss)\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Training Loss: {epoch_loss}\")\n",
    "\n",
    "    torch.save({\n",
    "         'model': model.state_dict(),\n",
    "         'optimizer': optimizer.state_dict(),\n",
    "         'lr_scheduler': lr_scheduler.state_dict(),\n",
    "         'epoch': epoch,\n",
    "     }, config.checkpoint)\n",
    "\n",
    "    validation_loss = evaluate(model, criterion, data_loader_val, device)\n",
    "    val_loss_hist.append(validation_loss)\n",
    "    print(f\"Validation Loss: {validation_loss}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff6ea504100>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.grid()\n",
    "plt.plot(train_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edward: note this makes a new caption as (<S>, 0, ..., 0) shouldn't we want as (<S>, <S>, ..., <S>)?\n",
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "\n",
    "    return caption_template, mask_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_report(captions):\n",
    "    all_reports = []\n",
    "    for report in captions:\n",
    "        if (report == word2ind[\"</s>\"]).any():\n",
    "            end_index = (report == word2ind[\"</s>\"]).nonzero()[0][0]\n",
    "            report = report[:end_index+1]\n",
    "        one_report = list(map(lambda x: ind2word[str(x)], report))\n",
    "        all_reports.append(one_report)\n",
    "    return all_reports\n",
    "\n",
    "def reports_to_sentence(reports):\n",
    "    return [' '.join(r) for r in make_report(reports)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(images):\n",
    "    all_captions = []\n",
    "    model.eval()\n",
    "    for i in range(len(images)):\n",
    "        image = images[i:i+1]\n",
    "        caption, cap_mask = create_caption_and_mask(\n",
    "            config.pad_token_id, config.max_position_embeddings)\n",
    "        for i in range(config.max_position_embeddings - 1):\n",
    "            predictions = model(image, caption, cap_mask)\n",
    "            predictions = predictions[:, i, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "            caption[:, i+1] = predicted_id[0]\n",
    "            cap_mask[:, i+1] = False\n",
    "            \n",
    "            if predicted_id[0] == word2ind[\"</s>\"]:\n",
    "                break\n",
    "\n",
    "        all_captions.append(caption.numpy())\n",
    "#     return make_report(all_captions)\n",
    "    return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, note, note_mask = next(iter(data_loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S> \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_np = np.asarray(report).squeeze(1)\n",
    "reports_to_sentence(report_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_to_sentence(np.asarray(note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(image[0].unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
