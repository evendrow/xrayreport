{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_matrix_from_cnn import *\n",
    "from get_notes_and_image_paths import *\n",
    "from get_word_embeddings import *\n",
    "from image_feature_dataset import *\n",
    "from load_glove_840B_300d import *\n",
    "from make_study_dictionary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = main_get_notes_and_image_paths(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 23094  words loaded!\n"
     ]
    }
   ],
   "source": [
    "words = main_get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-82-956a5b7f65c3>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-82-956a5b7f65c3>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with a linear embedding, positional encoder, encoder, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ndims=1024, wdim=300, vsize=23100, src_pad_inx, pdropout=0.3, device, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.positional_encoding_densenet = PositionalEncoding(wdim, dropout=0.1)\n",
    "        self.positional_encoding_chexpert = PositionalEncoding(wdim, dropout=0.1)\n",
    "        self.embedding_encoder_densenet = LinearEncoding(ndims, wdim, pdropout)\n",
    "        self.embedding_encoder_chexpert = LinearEncoding(ndims, wdim, pdropout)\n",
    "        self.positional_decoding = PositionalEncoding(wdim, dropout=0.1)\n",
    "        self.embedding_decoder = nn.Embedding(vocab_size, wdim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            wdim, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(wdim, vsize)\n",
    "        self.dropout = nn.Dropout(pdropout)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEncoding(nn.Module):\n",
    "    def __init__(self, ndims, embedding_size=300, dropout_p=0.3):\n",
    "        super(LinearEncoding, self).__init__()\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(ndims, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the linear encoder model (required)\n",
    "        Shape of x: (length of sequence for example, number of examples, ndims)\n",
    "            output: (length of sequence for example, number of examples, embedding_size)\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
