{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from catr.configuration import Config\n",
    "from catr.models.utils import NestedTensor, nested_tensor_from_tensor_list, get_rank\n",
    "from catr.models.backbone import build_backbone\n",
    "from catr.models.transformer_double_encoder import build_transformer_double_encoder\n",
    "from catr.models.position_encoding import PositionEmbeddingSine\n",
    "from catr.models.caption import MLP\n",
    "\n",
    "import json\n",
    "\n",
    "from dataset.dataset import ImageDoubleFeatureDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_ethan_double_encoder import *\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), \"catr\"))\n",
    "from engine_double_encoder import train_one_epoch_double_encoder, evaluate_double_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Embeddings\n",
    "words = np.load(\"glove_embed.npy\")\n",
    "with open('word2ind.json') as json_file: \n",
    "    word2ind = json.load(json_file) \n",
    "with open('ind2word.json') as json_file: \n",
    "    ind2word = json.load(json_file) \n",
    "    \n",
    "# Set up config file\n",
    "config = Config()\n",
    "config.feature_dim = 1024\n",
    "config.pad_token_id = word2ind[\"<S>\"]\n",
    "config.hidden_dim = 300\n",
    "config.nheads = 10\n",
    "config.batch_size = 16\n",
    "config.enc_layers = 6\n",
    "config.vocab_size = words.shape[0]\n",
    "config.checkpoint = './checkpoint_double_TEST.pth'\n",
    "config.dir = '../mimic_features_double'\n",
    "config.__dict__[\"pre_embed\"] = torch.from_numpy(words)\n",
    "config.__dict__[\"encoder_type\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Device: cuda\n",
      "Number of params: 45820808\n"
     ]
    }
   ],
   "source": [
    "model, criterion = main(config)\n",
    "model = model.float()\n",
    "device = torch.device(config.device)\n",
    "model.to(device)\n",
    "\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model.named_parameters(\n",
    "    ) if \"backbone\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": config.lr_backbone,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "        param_dicts, lr=config.lr, weight_decay=config.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config.lr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 253893\n",
      "Val: 1196\n"
     ]
    }
   ],
   "source": [
    "dataset_train = ImageDoubleFeatureDataset(config, mode='train')\n",
    "dataset_val = ImageDoubleFeatureDataset(config, mode='val')\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, config.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "        dataset_train, batch_sampler=batch_sampler_train, num_workers=config.num_workers)\n",
    "data_loader_val = DataLoader(dataset_val, config.batch_size,\n",
    "                                 sampler=sampler_val, drop_last=False, num_workers=config.num_workers)\n",
    "print(f\"Train: {len(dataset_train)}\")\n",
    "print(f\"Val: {len(dataset_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint...\n",
      "Start Training..\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"./checkpoint_double_10_tf.pth\"):\n",
    "    print(\"Loading Checkpoint...\")\n",
    "    checkpoint = torch.load(\"./checkpoint_double_10_tf.pth\", map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    config.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(\"Start Training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "train_bleu_hist = []\n",
    "val_bleu_hist = []\n",
    "\n",
    "for epoch in range(config.start_epoch, 20):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    epoch_loss, train_bleu_score = train_one_epoch_double_encoder(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch, config.clip_max_norm, word2ind)\n",
    "    train_loss_hist.append(epoch_loss)\n",
    "    train_bleu_hist.append(train_bleu_score)\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Training Bleu Score: {train_bleu_score}\")\n",
    "    print(f\"Training Loss: {epoch_loss}\")\n",
    "    \n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }, config.checkpoint)\n",
    "\n",
    "    validation_loss, val_bleu_score = evaluate_double_encoder(model, criterion, data_loader_val, device, word2ind)\n",
    "    val_loss_hist.append(validation_loss)\n",
    "    val_bleu_hist.append(val_bleu_score)\n",
    "    print(f\"Validation Bleu Score: {val_bleu_score}\")\n",
    "    print(f\"Validation Loss: {validation_loss}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edward: note this makes a new caption as (<S>, 0, ..., 0) shouldn't we want as (<S>, <S>, ..., <S>)?\n",
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "\n",
    "    return caption_template, mask_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_report(captions):\n",
    "    all_reports = []\n",
    "    for report in captions:\n",
    "        if (report == word2ind[\"</s>\"]).any():\n",
    "            end_index = (report == word2ind[\"</s>\"]).nonzero()[0][0]\n",
    "            report = report[:end_index+1]\n",
    "        one_report = list(map(lambda x: ind2word[str(x)], report))\n",
    "        all_reports.append(one_report)\n",
    "    return all_reports\n",
    "\n",
    "def reports_to_sentence(reports):\n",
    "    return [' '.join(r) for r in make_report(reports)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(images):\n",
    "    all_captions = []\n",
    "    model.eval()\n",
    "    for i in range(len(images[0])):\n",
    "        image1 = images[0][i:i+1].to(device)\n",
    "        image2 = images[1][i:i+1].to(device)\n",
    "        caption, cap_mask = create_caption_and_mask(\n",
    "            config.pad_token_id, config.max_position_embeddings)\n",
    "        for i in range(config.max_position_embeddings - 1):\n",
    "            predictions = model(image1, image2, caption, cap_mask)\n",
    "            predictions = predictions[:, i, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "            caption[:, i+1] = predicted_id[0]\n",
    "            cap_mask[:, i+1] = False\n",
    "            \n",
    "            if predicted_id[0] == word2ind[\"</s>\"]:\n",
    "                break\n",
    "\n",
    "        all_captions.append(caption.numpy())\n",
    "#     return make_report(all_captions)\n",
    "    return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sample_bleu4 = []\n",
    "sample_bleu3 = []\n",
    "sample_bleu2 = []\n",
    "sample_bleu1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = iter(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, note, note_mask = next(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_np = np.asarray(report).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score:  0.2574819918958893 0.11329171097764855 6.010168564233855e-104 4.377547941707098e-155\n",
      "Bleu score:  0.26021690648467544 0.10862066616770275 0.0654378833045959 1.3279403427149402e-78\n",
      "Bleu score:  0.32692307692307687 0.21182963643408087 0.12152840862513396 0.07779637090949697\n",
      "Bleu score:  0.16318055318565106 0.12639911298258805 0.09372411919266695 1.7305017688546433e-78\n",
      "Bleu score:  0.2557647735180994 0.09884262861234175 5.674154754883453e-104 4.299120532299612e-155\n",
      "Bleu score:  0.31281602148329835 0.21197381067415416 0.1667161816753367 0.12684067851857955\n",
      "Bleu score:  0.4089086287241437 0.2932435749928753 0.2103531207276943 0.1269215692088095\n",
      "Bleu score:  0.2653707856378107 0.11990727389027557 0.06424663373279794 1.503880657590319e-78\n",
      "Bleu score:  0.11927262468906635 0.07040973772670185 3.6946708052182934e-104 2.6763768211153003e-155\n",
      "Bleu score:  0.03693438612077796 0.017097298521688287 8.965692786400731e-105 6.492501115498954e-156\n",
      "Bleu score:  0.3663840366994972 0.23582319672943652 0.142898450412351 0.09442524558580725\n",
      "Bleu score:  0.1348782709381234 0.08571364528704041 0.06958913174303114 0.059145639775565036\n",
      "Bleu score:  0.3377767263046493 0.26377929113616677 0.18600297417744255 3.102916596926826e-78\n",
      "Bleu score:  0.2982456140350877 0.16318416798729055 0.07852316288619941 1.8116923322527955e-78\n",
      "Bleu score:  0.16393442622950818 0.05227083734893168 3.9320688651557675e-104 3.4103734949033603e-155\n",
      "Bleu score:  0.4762248034116116 0.32899956241588024 0.21798932103841054 0.151240443751577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for index in range(config.batch_size):\n",
    "    truth = reports_to_sentence(np.asarray(note[:,:]))[index]\n",
    "    generated = reports_to_sentence(report_np)[index]\n",
    "    truth = truth.replace(\"<S>\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"  \", \" \").split(\" \")\n",
    "    generated = generated.replace(\"<S>\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"  \", \" \").split(\" \")\n",
    "    truth = [y for y in truth if y != ''] \n",
    "    generated = [y for y in generated if y != ''] \n",
    "    bs4 = nltk.translate.bleu_score.sentence_bleu([truth], generated, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    bs3 = nltk.translate.bleu_score.sentence_bleu([truth], generated, weights=[1./3., 1./3., 1./3.])\n",
    "    bs2 = nltk.translate.bleu_score.sentence_bleu([truth], generated, weights=[0.5, 0.5])\n",
    "    bs1 = nltk.translate.bleu_score.sentence_bleu([truth], generated, weights=[1.])\n",
    "    sample_bleu4.append(bs4)\n",
    "    sample_bleu3.append(bs3)\n",
    "    sample_bleu2.append(bs2)\n",
    "    sample_bleu1.append(bs1)\n",
    "    print(\"Bleu score: \", bs1, bs2, bs3, bs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03977312173436471"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sample_bleu4)/len(sample_bleu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_bleu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_np = np.asarray(report).squeeze(1)\n",
    "reports_to_sentence(report_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, note, note_mask = next(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S> Subtle patchy opacity along the left heart border on the frontal view , not substantiated on the lateral view , may be due to <unk> scarring or epicardial fat pad , less likely consolidation . <s> No focal consolidation seen elsewhere . <s> There is no pleural effusion or pneumothorax . <s> Cardiac and mediastinal silhouettes are stable . <s> Hilar contours are stable . <s> No overt pulmonary edema is seen . <s> Chronic changes at the right acromioclavicular joint are not well assessed . <s> </s>',\n",
       " '<S> AP upright and lateral views of the chest provided . <s> There is no focal consolidation , effusion , or pneumothorax . <s> The cardiomediastinal silhouette is normal . <s> Imaged osseous structures are intact . <s> No free air below the right hemidiaphragm is seen . <s> </s>',\n",
       " '<S> No focal consolidation is seen . <s> There is elevation of the mid to posterior left hemidiaphragm with minimal blunting of the left costophrenic angle without a definite pleural effusion seen on the lateral view . <s> No evidence of pneumothorax is seen . <s> The cardiac and mediastinal silhouettes are unremarkable . <s> Evidence of DISH is seen along the spine . <s> No displaced fracture is seen . <s> </s>',\n",
       " '<S> In comparison with the study of ___ , there is continued opacification at the left base most likely reflecting pleural effusion and volume loss in the lower lobe . <s> Mild blunting of the right costophrenic angle persists . <s> No evidence of vascular congestion . <s> Right <unk> catheter remains in place . <s> </s>',\n",
       " '<S> The lungs are clear . <s> The cardiomediastinal silhouette is within normal limits . <s> No acute osseous abnormalities . <s> </s>',\n",
       " '<S> No previous studies for comparison . <s> The heart size is within normal limits . <s> Lungs are grossly clear without definite consolidation , pleural effusions , or signs for acute pulmonary edema . <s> There are no pneumothoraces . <s> </s>',\n",
       " '<S> The lungs are clear without consolidation or edema . <s> The mediastinum is unremarkable . <s> The cardiac silhouette is within normal limits for size . <s> No effusion or pneumothorax is noted . <s> The visualized osseous structures are unremarkable . <s> </s>',\n",
       " '<S> PA and lateral views of the chest . <s> No prior . <s> The lungs are clear . <s> <unk> silhouette is normal . <s> Osseous structures are unremarkable . <s> </s>',\n",
       " '<S> Interval increase in moderate-sized right pleural effusion , and right lower lobe opacity with new right upper lobe heterogeneous opacity . <s> Unchanged left apical pleural thickening and scarring . <s> No interval change in the dense retrocardiac opacity obscuring the left hemidiaphragm which represents a <unk> hernia . <s> No pneumothorax or pulmonary edema . <s> Heart size is partially obscured by the pleural parenchymal process . <s> Mediastinal contour and hila are normal . <s> No bony abnormality . <s> </s>',\n",
       " '<S> Ill-defined patchy opacities are seen in the right lung base with an associated small right pleural effusion , which is also confirmed in the lateral view . <s> A dense left-sided retrocardiac opacity abutting the left hemidiaphragm is unchanged since at least ___ compatible with a <unk> hernia . <s> A small left pleural effusion is also likely present . <s> There is <unk> <unk> scarring , more conspicuous in the left apex . <s> No other focal opacities are identified . <s> Mild cardiomegaly is unchanged from prior . <s> There is no pneumothorax . <s> </s>',\n",
       " '<S> Allowing for differences in technique and projection , there has been minimal change in the appearance of the chest except for apparent slight increase in bilateral pleural effusions , now moderate on the right and small to moderate on the left . <s> </s>',\n",
       " '<S> AP portable upright view of the chest . <s> Extensive intrathoracic calcifications are again seen , better localized on the chest CT examination from ___ . <s> The heart size is top normal . <s> A tracheostomy tube is appropriately positioned . <s> A right PICC terminates at the caval atrial junction . <s> Again seen are bilateral pulmonary parenchymal opacities , with interval improvement along the right mid and lower zones since the ___ radiograph . <s> Opacities across the left lung are unchanged . <s> There is no pneumothorax . <s> Small bilateral pleural effusions are stable . <s> <unk> , </s>',\n",
       " '<S> The tracheostomy tube is unchanged in position and terminates approximately 4.8 cm above the carina . <s> The right PICC line terminates in the distal SVC . <s> There is no significant change in the lungs when compared to ___ . <s> There are several parenchymal calcifications which were characterized on the most recent CT scan . <s> Again noted are diffuse infiltrative parenchymal opacities , right worse than left ; this is largely due to pulmonary edema and the right-sided pleural effusion , but underlying pneumonia cannot be excluded . <s> The mediastinum is wide , which was noted as far back as the outside hospital <unk> from ___ . <s> No acute osseous abnormalities . <s> </s>',\n",
       " '<S> Multiple calcified pulmonary nodules and calcified lymph nodes within the neck . <s> Severe degenerative changes of the glenohumeral joints . <s> Bilateral pleural effusions with bibasilar atelectasis . <s> Developing bibasilar consolidation is difficult to exclude . <s> No pneumothorax . <s> </s>',\n",
       " '<S> AP portable semi upright view of the chest . <s> Multiple calcified lymph nodes again seen projecting over the chest and neck . <s> The previously noted tracheostomy tube is no longer seen . <s> Calcified pleural plaque along the right hemidiaphragm noted along with multiple bilateral calcified pulmonary nodules . <s> A small right pleural effusion is likely present . <s> No convincing signs of pneumonia . <s> The cardiomediastinal silhouette appears grossly within normal limits . <s> Severe degenerative disease at both shoulders is again noted . <s> </s>',\n",
       " '<S> An ET tube is present approximately 3.6 cm above the carina . <s> The enteric tube is present the distal tip off the film . <s> There is no pneumothorax . <s> There are small bilateral effusions . <s> Dense calcified opacities in both upper lung fields and hila are noted , consistent with prior history of tuberculosis . <s> Atelectasis or consolidation of the lung bases are noted . <s> Reticular changes are also noted , which may be acute or chronic . <s> </s>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_to_sentence(np.asarray(note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
